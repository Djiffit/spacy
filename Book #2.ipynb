{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "COUNTRIES = ['Afghanistan', 'Åland Islands', 'Albania', 'Algeria', 'American Samoa', 'Andorra', 'Angola', 'Anguilla', 'Antarctica', 'Antigua and Barbuda', 'Argentina', 'Armenia', 'Aruba', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bermuda', 'Bhutan', 'Bolivia (Plurinational State of)', 'Bonaire, Sint Eustatius and Saba', 'Bosnia and Herzegovina', 'Botswana', 'Bouvet Island', 'Brazil', 'British Indian Ocean Territory', 'United States Minor Outlying Islands', 'Virgin Islands (British)', 'Virgin Islands (U.S.)', 'Brunei Darussalam', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Cabo Verde', 'Cayman Islands', 'Central African Republic', 'Chad', 'Chile', 'China', 'Christmas Island', 'Cocos (Keeling) Islands', 'Colombia', 'Comoros', 'Congo', 'Congo (Democratic Republic of the)', 'Cook Islands', 'Costa Rica', 'Croatia', 'Cuba', 'Curaçao', 'Cyprus', 'Czech Republic', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Ethiopia', 'Falkland Islands (Malvinas)', 'Faroe Islands', 'Fiji', 'Finland', 'France', 'French Guiana', 'French Polynesia', 'French Southern Territories', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Gibraltar', 'Greece', 'Greenland', 'Grenada', 'Guadeloupe', 'Guam', 'Guatemala', 'Guernsey', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Heard Island and McDonald Islands', 'Holy See', 'Honduras', 'Hong Kong', 'Hungary', 'Iceland', 'India', 'Indonesia', \"Côte d'Ivoire\", 'Iran (Islamic Republic of)', 'Iraq', 'Ireland', 'Isle of Man', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jersey', 'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati', 'Kuwait', 'Kyrgyzstan', \"Lao People's Democratic Republic\", 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Macao', 'Macedonia (the former Yugoslav Republic of)', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Martinique', 'Mauritania', 'Mauritius', 'Mayotte', 'Mexico', 'Micronesia (Federated States of)', 'Moldova (Republic of)', 'Monaco', 'Mongolia', 'Montenegro', 'Montserrat', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nauru', 'Nepal', 'Netherlands', 'New Caledonia', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Niue', 'Norfolk Island', \"Korea (Democratic People's Republic of)\", 'Northern Mariana Islands', 'Norway', 'Oman', 'Pakistan', 'Palau', 'Palestine, State of', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', 'Pitcairn', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar', 'Republic of Kosovo', 'Réunion', 'Romania', 'Russian Federation', 'Rwanda', 'Saint Barthélemy', 'Saint Helena, Ascension and Tristan da Cunha', 'Saint Kitts and Nevis', 'Saint Lucia', 'Saint Martin (French part)', 'Saint Pierre and Miquelon', 'Saint Vincent and the Grenadines', 'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Sint Maarten (Dutch part)', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Georgia and the South Sandwich Islands', 'Korea (Republic of)', 'South Sudan', 'Spain', 'Sri Lanka', 'Sudan', 'Suriname', 'Svalbard and Jan Mayen', 'Swaziland', 'Sweden', 'Switzerland', 'Syrian Arab Republic', 'Taiwan', 'Tajikistan', 'Tanzania, United Republic of', 'Thailand', 'Timor-Leste', 'Togo', 'Tokelau', 'Tonga', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Turkmenistan', 'Turks and Caicos Islands', 'Tuvalu', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom of Great Britain and Northern Ireland', 'United States of America', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Venezuela (Bolivarian Republic of)', 'Viet Nam', 'Wallis and Futuna', 'Western Sahara', 'Yemen', 'Zambia', 'Zimbabwe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash value:  3197928453018144401\n",
      "String value of hash: coffee\n",
      "coffee 3197928453018144401 True\n"
     ]
    }
   ],
   "source": [
    "nlp = English()\n",
    "\n",
    "doc = nlp('I love coffee, all coffees should be treated as equal')\n",
    "\n",
    "cof_hash = nlp.vocab.strings['coffee']\n",
    "print('Hash value: ', cof_hash)\n",
    "cof_string = nlp.vocab.strings[cof_hash]\n",
    "print('String value of hash:', cof_string)\n",
    "\n",
    "lexeme = doc.vocab['coffee']\n",
    "\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5439657043933447811\n",
      "cat\n",
      "380\n",
      "PERSON\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp('I have a cat')\n",
    "\n",
    "cat_hash = nlp.vocab.strings['cat']\n",
    "print(cat_hash)\n",
    "\n",
    "cat_string = nlp.vocab.strings[cat_hash]\n",
    "print(cat_string)\n",
    "\n",
    "doc = nlp('David Bowie is a PERSON')\n",
    "\n",
    "person_hash = nlp.vocab.strings['PERSON']\n",
    "print(person_hash)\n",
    "\n",
    "person_string = nlp.vocab.strings[person_hash]\n",
    "print(person_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German Bowie hash: 2644858412616767388, English Bowie: 2644858412616767388, Ger word: Bowie, En word: Bowie\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.de import German\n",
    "\n",
    "nlp_de = German()\n",
    "nlp = English()\n",
    "\n",
    "doc_de = nlp_de('David Bowie does exist!')\n",
    "doc = nlp('David Bowie does exist!')\n",
    "\n",
    "# Hash gives same hash for both languages\n",
    "bowie_id = nlp.vocab.strings['Bowie']\n",
    "bowie_id2 = nlp_de.vocab.strings['Bowie']\n",
    "\n",
    "# If word does not exist in the doc this will throw an error\n",
    "print(f'German Bowie hash: {bowie_id2}, English Bowie: {bowie_id}, Ger word: {nlp_de.vocab.strings[bowie_id]}, En word: {nlp.vocab.strings[bowie_id2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Hello\n",
      "1 world\n",
      "2 !\n",
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "nlp = English()\n",
    "words = 'Hello world !'.split()\n",
    "spaces = [True, False, False]\n",
    "\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.i, token.text)\n",
    "    \n",
    "span = Span(doc, 0, 2, label='Greetings my dude')\n",
    "\n",
    "doc.ents = [span]\n",
    "\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy is cool!\n",
      "Go, get started!\n",
      "Oh, really?!\n",
      "David Bowie PERSON\n",
      "Found a proper noun before verb: Berlin\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "words = [\"spaCy\", \"is\", \"cool\", \"!\"]\n",
    "spaces = [True, True, False, False]\n",
    "\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "print(doc.text)\n",
    "\n",
    "words = [\"Go\", \",\", \"get\", \"started\", \"!\"]\n",
    "spaces = [False, True, True, False, False]\n",
    "\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "print(doc.text)\n",
    "\n",
    "words = 'Oh , really ? !'.split()\n",
    "spaces = [False, True, False, False, False]\n",
    "\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "print(doc.text)\n",
    "\n",
    "words = [\"I\", \"like\", \"David\", \"Bowie\"]\n",
    "spaces = [True, True, True, False]\n",
    "\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "span = Span(doc, 2, 4, label='PERSON')\n",
    "\n",
    "doc.ents = [span]\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent, ent.label_)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin is a nice city\")\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ == 'PROPN':\n",
    "        if doc[token.i + 1].pos_ == 'VERB':\n",
    "            print('Found a proper noun before verb:', token.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word vectors\n",
    "\n",
    "nlp = spacy.load('en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8627204117787385\n",
      "0.7369546\n",
      "0.6470851511976461\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc1 = nlp('I like fast food')\n",
    "doc2 = nlp('I like pizza')\n",
    "\n",
    "print(doc1.similarity(doc2))\n",
    "\n",
    "doc = nlp('I like pizza and pasta')\n",
    "\n",
    "print(doc[2].similarity(doc[4]))\n",
    "\n",
    "print(doc.similarity(nlp('Food')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "0.8789265574516525\n",
      "0.22325331\n",
      "0.75173926\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Two bananas in pyjamas')\n",
    "\n",
    "b_vec = doc[1].vector\n",
    "print(len(b_vec))\n",
    "\n",
    "doc1 = nlp(\"It's a warm summer day\")\n",
    "doc2 = nlp(\"It's sunny outside\")\n",
    "\n",
    "similarity = doc1.similarity(doc2)\n",
    "\n",
    "print(similarity)\n",
    "\n",
    "doc = nlp(\"TV and books\")\n",
    "\n",
    "t1, t2 = doc[0], doc[2]\n",
    "\n",
    "sim = t1.similarity(t2)\n",
    "print(sim)\n",
    "\n",
    "\n",
    "doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
    "\n",
    "span1 = doc[3:5]\n",
    "span2 = doc[-4:-1]\n",
    "\n",
    "similarity = span1.similarity(span2)\n",
    "\n",
    "print(similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match:  golden retriever\n",
      "Root token: retriever\n",
      "Root head token: have\n",
      "Previous token: a DET\n",
      "Match Golden Retriever\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('DOG', None, [{'LOWER': 'golden'}, {'LOWER': 'retriever'}])\n",
    "doc = nlp('I have a golden retriever')\n",
    "\n",
    "for match_, start ,end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    \n",
    "    print('Match: ', span.text)\n",
    "    print('Root token:', span.root.text)\n",
    "    print('Root head token:', span.root.head.text)\n",
    "    print('Previous token:', doc[start - 1].text, doc[start - 1].pos_)\n",
    "    \n",
    "    \n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp('Golden Retriever')\n",
    "\n",
    "matcher.add('DOG', None, pattern)\n",
    "\n",
    "doc = nlp('I have a Golden Retriever')\n",
    "\n",
    "for match, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print('Match', span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon Prime\n",
      "ad-free viewing\n",
      "Amazon Prime\n",
      "ad-free viewing\n",
      "ad-free viewing\n",
      "ad-free viewing\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\n",
    "    \"Twitch Prime, the perks program for Amazon Prime members offering free \"\n",
    "    \"loot, games and other benefits, is ditching one of its best features: \"\n",
    "    \"ad-free viewing. According to an email sent out to Amazon Prime members \"\n",
    "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
    "    \"Prime for new members, beginning on September 14. However, members with \"\n",
    "    \"existing annual subscriptions will be able to continue to enjoy ad-free \"\n",
    "    \"viewing until their subscription comes up for renewal. Those with \"\n",
    "    \"monthly subscriptions will have access to ad-free viewing until October 15.\"\n",
    ")\n",
    "\n",
    "pattern1 = [{\"LOWER\": \"amazon\"}, {\"IS_TITLE\": True, \"POS\": \"PROPN\"}]\n",
    "pattern2 = [{\"TEXT\": \"ad\"}, {'TEXT': '-'}, {'TEXT': 'free'}, {'POS': 'NOUN'}]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "matcher.add('AMAZON', None, pattern1)\n",
    "matcher.add(\"AD-FREE\", None, pattern2)\n",
    "\n",
    "for match, start, end in matcher(doc):\n",
    "    print(doc[start:end])\n",
    "    \n",
    "# for token in doc:\n",
    "#     print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Åland Islands, Czech Republic, Slovakia, American Samoa, Samoa, Antigua and Barbuda]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "doc = nlp('Åland Islands and Czech Republic may help Slovakia protect its airspace despite American Samoa being so vehemontly against it alongisde Antigua and Barbuda.')\n",
    "\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add('COUNTRY', None, *patterns)\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "print([doc[start:end] for match, start, end in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namibia --> Namibia\n",
      "South --> South Africa\n",
      "Cambodia --> Cambodia\n",
      "Kuwait --> Kuwait\n",
      "Somalia --> Somalia\n",
      "Haiti --> Haiti\n",
      "Mozambique --> Mozambique\n",
      "Somalia --> Somalia\n",
      "Rwanda --> Rwanda\n",
      "Singapore --> Singapore\n",
      "Sierra --> Sierra Leone\n",
      "Afghanistan --> Afghanistan\n",
      "Iraq --> Iraq\n",
      "Sudan --> Sudan\n",
      "Congo --> Congo\n",
      "Haiti --> Haiti\n",
      "[('Namibia', 'GPE'), ('South Africa', 'GPE'), ('Cambodia', 'GPE'), ('Kuwait', 'GPE'), ('Somalia', 'GPE'), ('Haiti', 'GPE'), ('Mozambique', 'GPE'), ('Somalia', 'GPE'), ('Rwanda', 'GPE'), ('Singapore', 'GPE'), ('Sierra Leone', 'GPE'), ('Afghanistan', 'GPE'), ('Iraq', 'GPE'), ('Sudan', 'GPE'), ('Congo', 'GPE'), ('Haiti', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "with open('country-text.txt') as f:\n",
    "    TEXT = f.read()\n",
    "\n",
    "nlp_e = English()\n",
    "matcher = PhraseMatcher(nlp_e.vocab)\n",
    "patterns = list(nlp_e.pipe(COUNTRIES))\n",
    "matcher.add('COUNTRY', None, *patterns)\n",
    "doc = nlp_e(TEXT)\n",
    "\n",
    "\n",
    "for match, start, end in matcher(doc):\n",
    "    span = Span(doc, start, end, label='GPE')\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "    print(span.root.head.text, '-->', span)\n",
    "\n",
    "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == \"GPE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
